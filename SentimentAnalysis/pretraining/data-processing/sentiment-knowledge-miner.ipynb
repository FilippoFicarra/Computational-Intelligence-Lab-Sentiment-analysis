{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:02:52.976282700Z",
     "start_time": "2023-07-16T15:02:51.409185700Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from utility_functions import *\n",
    "import os\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:02:55.248015700Z",
     "start_time": "2023-07-16T15:02:55.215805300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "PATH_OCCURRENCES = directory + \"/data/sentiment-knowledge/all-words-with-occurrences.csv\"\n",
    "PATH_SEEDS_POSITIVE = directory + \"/data/sentiment-knowledge/seeds-positive.txt\"\n",
    "PATH_SEEDS_NEGATIVE = directory + \"/data/sentiment-knowledge/seeds-negative.txt\"\n",
    "PATH_PMI = directory + \"/data/sentiment-knowledge/pmi.csv\"\n",
    "PATH_CO_OCCURRENCES = directory + \"/data/sentiment-knowledge/co-occurrences.csv\"\n",
    "CHUNK_SIZE = 100000\n",
    "PATH_DATASET = directory + \"/data/datasets/dataset-cleaned-no-unknown.json\"\n",
    "FIRST_COLUMN_OCC = \"word\"\n",
    "SECOND_COLUMN_OCC = \"occurrences\"\n",
    "DICTIONARY_US = enchant.Dict(\"en-US\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:02:55.963316500Z",
     "start_time": "2023-07-16T15:02:55.909407400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = spark.read.json(PATH_DATASET)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:03:02.110107600Z",
     "start_time": "2023-07-16T15:02:56.767426100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Get list of words\n",
    "df_words = pd.read_csv(PATH_OCCURRENCES, keep_default_na=False, dtype={\"word\": str, \"occurrences\": int})\n",
    "\n",
    "# Get seeds\n",
    "seeds = []\n",
    "with open(PATH_SEEDS_POSITIVE, 'r') as f1, open(PATH_SEEDS_NEGATIVE, 'r') as f2:\n",
    "    read_lines(f1, seeds)\n",
    "    read_lines(f2, seeds)\n",
    "\n",
    "# Create dictionary for fast lookup of seed words and for occurrences. Get also the total number of occurrences\n",
    "tot_occurrences = 0\n",
    "occurences_dict = {}\n",
    "seeds_dict = {}\n",
    "checked_dict = {}\n",
    "for row in df_words.itertuples():\n",
    "    # Increase total number of occurrences\n",
    "    tot_occurrences += row.occurrences\n",
    "    # Add word to dictionary of occurrences\n",
    "    occurences_dict[row.word] = row.occurrences\n",
    "    # Add word to dictionary of checked\n",
    "    checked_dict[row.word] = DICTIONARY_US.check(row.word)\n",
    "    # Add word to dictionary for seeds identification\n",
    "    if row.word in seeds:\n",
    "        seeds_dict[row.word] = True\n",
    "    else:\n",
    "        seeds_dict[row.word] = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:03:23.111223100Z",
     "start_time": "2023-07-16T15:03:03.150171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "save_list_to_csv(dataset.rdd.map(lambda obj: tokenize_with_sequences(remove_symbols_before_tokenization(obj[\"reviewText\"])))\\\n",
    "            .flatMap(lambda x: [(pair, 1) for pair in get_pairs_word_seed(x, seeds_dict, occurences_dict, checked_dict)])\\\n",
    "            .reduceByKey(lambda x, y: x + y)\\\n",
    "            .map(lambda pair: (pair[0][0], pair[0][1], pmi(pair[1], occurences_dict[pair[0][0]], occurences_dict[pair[0][1]], tot_occurrences)))\\\n",
    "            .sortBy(lambda pair: pair[2], ascending=False)\\\n",
    "            .collect(), PATH_PMI, ['word1', 'word2', 'pmi'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T15:13:31.514406700Z",
     "start_time": "2023-07-16T15:03:47.647990400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
