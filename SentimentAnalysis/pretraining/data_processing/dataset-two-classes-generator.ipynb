{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generator for dataset with two classes\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:15:19.502441600Z",
     "start_time": "2023-07-21T16:15:18.010561800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define paths\n",
    "path_cleaned_unknown = directory + \"/data/datasets/dataset-cleaned-no-unknown.json\"\n",
    "path_two_classes = directory + \"/data/datasets/dataset-two-classes.json\"\n",
    "path_two_classes_directory = directory + \"/data/datasets/dataset-two-classes\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:15:21.148157200Z",
     "start_time": "2023-07-21T16:15:21.123608600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xmx32g -Xms12g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:15:22.510260700Z",
     "start_time": "2023-07-21T16:15:22.495278600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset = spark.read.json(path_cleaned_unknown, schema=\"overall float, reviewText string\")\n",
    "\n",
    "# Split rdd into multiple rdds\n",
    "split_rdds = dataset.randomSplit([0.1 for _ in range(0, 10)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:15:25.157809400Z",
     "start_time": "2023-07-21T16:15:23.419479Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create rdd of cleaned text\n",
    "rdds = []\n",
    "\n",
    "for df in split_rdds:\n",
    "    rdds.append(\n",
    "        df.rdd.filter(lambda obj: obj[\"overall\"] != 3.)\n",
    "        .map(lambda obj: {\"overall\": 0. if obj[\"overall\"] <= 2. else 1., \"reviewText\": obj[\"reviewText\"]})\n",
    "    )\n",
    "# Save cleaned dataset with unknown words\n",
    "for i in range(0, len(rdds)):\n",
    "    save_rdd_to_json_file(path_two_classes_directory + \"/two-classes\" + f\"{i}\", rdds[i])\n",
    "\n",
    "# Create dataset from files\n",
    "merge_files(path_two_classes_directory, path_two_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T16:17:01.482243300Z",
     "start_time": "2023-07-21T16:15:30.669036600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
