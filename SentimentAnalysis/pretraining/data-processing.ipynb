{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset extractor\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length, regexp_replace\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.default.parallelism\", \"25\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\\\n",
    "    .master('local[*]')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read dataset\n",
    "dataset = spark.read.format(\"json\").load(\"data/All_Amazon_Review.json\", schema=\"overall float, reviewText string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of examples to take and limit for number of characters.\n",
    "limit = 1000000\n",
    "limit_characters = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by overall\n",
    "dataset1 = dataset.filter(dataset[\"overall\"] == 1.0)\n",
    "\n",
    "# Filter by text size \n",
    "dataset1 = dataset1.filter(length(regexp_replace(dataset1[\"reviewText\"], \"\\s+\", \"\")) <= limit_characters)\n",
    "\n",
    "# Add a new column with an auto-incrementing ID\n",
    "dataset1 = dataset1.sample(False, limit / dataset1.count() if limit / dataset1.count() <= 1.0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by overall\n",
    "dataset2 = dataset.filter(dataset[\"overall\"] == 2.0)\n",
    "\n",
    "# Filter by text size \n",
    "dataset2 = dataset2.filter(length(regexp_replace(dataset2[\"reviewText\"], \"\\s+\", \"\")) <= limit_characters)\n",
    "\n",
    "# Add a new column with an auto-incrementing ID\n",
    "dataset2 = dataset2.sample(False, limit / dataset2.count() if limit / dataset2.count() <= 1.0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by overall\n",
    "dataset3 = dataset.filter(dataset[\"overall\"] == 3.0)\n",
    "\n",
    "# Filter by text size \n",
    "dataset3 = dataset3.filter(length(regexp_replace(dataset3[\"reviewText\"], \"\\s+\", \"\")) <= limit_characters)\n",
    "\n",
    "# Add a new column with an auto-incrementing ID\n",
    "dataset3 = dataset3.sample(False, limit / dataset3.count() if limit / dataset3.count() <= 1.0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by overall\n",
    "dataset4 = dataset.filter(dataset[\"overall\"] == 4.0)\n",
    "\n",
    "# Filter by text size \n",
    "dataset4 = dataset4.filter(length(regexp_replace(dataset4[\"reviewText\"], \"\\s+\", \"\")) <= limit_characters)\n",
    "\n",
    "# Add a new column with an auto-incrementing ID\n",
    "dataset4 = dataset4.sample(False, limit / dataset4.count() if limit / dataset4.count() <= 1.0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by overall\n",
    "dataset5 = dataset.filter(dataset[\"overall\"] == 5.0)\n",
    "\n",
    "# Filter by text size \n",
    "dataset5 = dataset5.filter(length(regexp_replace(dataset5[\"reviewText\"], \"\\s+\", \"\")) <= limit_characters)\n",
    "\n",
    "# Add a new column with an auto-incrementing ID\n",
    "dataset5 = dataset5.sample(False, limit / dataset5.count() if limit / dataset5.count() <= 1.0 else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "merged_dataset = dataset1.union(dataset2).union(dataset3).union(dataset4).union(dataset5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/output\"\n",
    "merged_dataset.repartition(1)\n",
    "merged_dataset.write.json(output_path, mode=\"overwrite\", lineSep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "directory = \"data/output\"\n",
    "output_file = 'data/dataset.jsonl'\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        outfile.write(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
