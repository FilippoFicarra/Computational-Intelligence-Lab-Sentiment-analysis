{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleaner for twitter test dataset\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import *\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define paths\n",
    "path_test_tweets = directory + \"/data/twitter-data/test_data.txt\"\n",
    "path_tweets = directory + \"/data/datasets/twitter-test.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xmx32g -Xms12g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_index_and_comma(string):\n",
    "    first_comma_index = string.index(',')\n",
    "    return string[first_comma_index + 1:]\n",
    "\n",
    "# Read dataset with test data\n",
    "dataset_test = spark.read.text(path_test_tweets).rdd.map(lambda x: {\"text\": remove_index_and_comma(x.value)})\n",
    "\n",
    "# Clean dataset\n",
    "dataset_test_cleaned = dataset_test.map(lambda obj: {\"text\": new_cleaning_function_twitter_dataset(obj[\"text\"])})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split words with less than 15 occurrences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute number of occurrences of words\n",
    "words_with_occurrences = dataset_test_cleaned.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(remove_symbols_before_tokenization(obj[\"text\"], True), True)]).reduceByKey(lambda x, y: x + y).collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split words\n",
    "import wordninja\n",
    "\n",
    "# Create keyword processor\n",
    "kp = KeywordProcessor()\n",
    "\n",
    "# Process list of words with occurrences\n",
    "for word in sorted(words_with_occurrences, key=lambda x: x[1]):\n",
    "    if word[1] <= 15:\n",
    "        if not contains_numbers(word[0]):\n",
    "            new_words = wordninja.split(word[0])\n",
    "            value = \" \".join(new_words)\n",
    "            kp.add_keyword(word[0], value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define keyword processor to replace <user> and <url>\n",
    "kp_special_tokens = KeywordProcessor()\n",
    "kp_special_tokens.add_keyword(\"<user>\", \"@USER\")\n",
    "kp_special_tokens.add_keyword(\"<url>\", \"HTTPURL\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create rdd of cleaned text without unknown words\n",
    "dataset_test_cleaned_with_split_text = dataset_test_cleaned.map(lambda obj: {\"text\": \" \".join(kp.replace_keywords(obj[\"text\"]).split())}).map(lambda obj: {\"text\": kp_special_tokens.replace_keywords(obj[\"text\"])})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove ids, i.e. words with both numbers and characters, and save dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get words with less than 10 occurrences\n",
    "words_to_analyze = dataset_test_cleaned_with_split_text.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(obj[\"text\"], True)]).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] <= 10).collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove words with one occurrence and words with less than 10 occurrences containing both numbers and characters\n",
    "kp_words_to_remove = KeywordProcessor()\n",
    "\n",
    "for word in words_to_analyze:\n",
    "    if word[1] <= 10:\n",
    "        if contains_numbers(word[0]) and not contains_numbers_and_x(word[0]):\n",
    "            kp_words_to_remove.add_keyword(word[0], \" \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create rdd with final dataset\n",
    "dataset_test_cleaned_final = dataset_test_cleaned_with_split_text.map(lambda obj: {\"text\": \" \".join(kp_words_to_remove.replace_keywords(obj[\"text\"]).split())})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Write dataset for test to file\n",
    "with open(path_tweets, \"a\") as f:\n",
    "    for item in dataset_test_cleaned_final.collect():\n",
    "            f.write(str(item).replace(\"'\", \"\\\"\") + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
