{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleaner for twitter dataset\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import *\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:11:47.428633200Z",
     "start_time": "2023-07-24T22:11:46.319984900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define paths\n",
    "path_neg_tweets = directory + \"/data/twitter-data/train_neg_full.txt\"\n",
    "path_pos_tweets = directory + \"/data/twitter-data/train_pos_full.txt\"\n",
    "path_tweets = directory + \"/data/datasets/twitter-cleaned.json\"\n",
    "path_all_occurrences = directory + \"/data/sentiment-knowledge/twitter-all-words-with-occurrences.csv\"\n",
    "path_tweets_no_unknown_directory = directory + \"/data/datasets/twitter\"\n",
    "path_tweets_no_unknown = directory + \"/data/datasets/twitter.json\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:11:47.444819200Z",
     "start_time": "2023-07-24T22:11:47.430565200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xmx32g -Xms12g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:11:50.219355800Z",
     "start_time": "2023-07-24T22:11:50.193502800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "dataset_neg = spark.read.text(path_neg_tweets).rdd.distinct().map(lambda x: {\"label\": 0, \"text\": x.value})\n",
    "dataset_pos = spark.read.text(path_pos_tweets).rdd.distinct().map(lambda x: {\"label\": 1, \"text\": x.value})\n",
    "\n",
    "# Merge\n",
    "dataset = dataset_neg.union(dataset_pos)\n",
    "\n",
    "# Split rdd into multiple rdds\n",
    "split_rdds = dataset.randomSplit([0.1 for _ in range(0,10)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:12:05.081490900Z",
     "start_time": "2023-07-24T22:12:02.922848600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Process dataset\n",
    "rdds = []\n",
    "\n",
    "for rdd in split_rdds:\n",
    "    rdds.append(\n",
    "        rdd.map(lambda obj: {\"label\": obj[\"label\"], \"text\": new_cleaning_function_twitter_dataset(obj[\"text\"])})\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:12:10.699869900Z",
     "start_time": "2023-07-24T22:12:10.674838300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(path_tweets, \"a\") as f:\n",
    "    for rdd in rdds:\n",
    "        for item in rdd.collect():\n",
    "            f.write(str(item).replace(\"'\", \"\\\"\") + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:17:27.212859300Z",
     "start_time": "2023-07-24T22:12:17.935227300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Occurences analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "dataset_cleaned = spark.read.json(path_tweets)\n",
    "\n",
    "# Split rdd into multiple rdds\n",
    "split_rdds_cleaned = dataset_cleaned.randomSplit([0.1 for _ in range(0, 10)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:17:49.725776800Z",
     "start_time": "2023-07-24T22:17:49.112855700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Compute list of words with number of occurrences\n",
    "words_occurrences_rdds = []\n",
    "\n",
    "for df in split_rdds_cleaned:\n",
    "    words_occurrences_rdds.append(\n",
    "        df.rdd.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(\n",
    "            remove_symbols_before_tokenization(obj[\"text\"], True), True)])  # Tokenize\n",
    "        .reduceByKey(lambda x, y: x + y)  # Add occurrences\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:17:58.664985300Z",
     "start_time": "2023-07-24T22:17:58.067537Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Merge rdds, then reduce by key to obtain final vocabulary with number of occurreces\n",
    "merged_rdd_occurrences = words_occurrences_rdds[0].union(words_occurrences_rdds[1])\n",
    "\n",
    "for i in range(2, len(words_occurrences_rdds)):\n",
    "    merged_rdd_occurrences = merged_rdd_occurrences.union(words_occurrences_rdds[i])\n",
    "\n",
    "words_with_occurrences = merged_rdd_occurrences.reduceByKey(lambda x, y: x + y).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:20:39.180187300Z",
     "start_time": "2023-07-24T22:18:09.835382500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Split words\n",
    "import wordninja\n",
    "import re\n",
    "\n",
    "# Create keyword processor\n",
    "kp = KeywordProcessor()\n",
    "\n",
    "# Process list of words with occurrences\n",
    "for word in sorted(words_with_occurrences, key=lambda x: x[1]):\n",
    "    if word[1] <= 15:\n",
    "        if not contains_numbers(word[0]):\n",
    "            new_words = wordninja.split(word[0])\n",
    "            value = \" \".join(new_words)\n",
    "            kp.add_keyword(word[0], value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:21:28.190551Z",
     "start_time": "2023-07-24T22:21:19.047552100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define keyword processor to replace <user> and <url>\n",
    "kp_special_tokens = KeywordProcessor()\n",
    "kp_special_tokens.add_keyword(\"<user>\", \"xxuser\")\n",
    "kp_special_tokens.add_keyword(\"<url>\", \"xxurl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:23:23.949592Z",
     "start_time": "2023-07-24T22:23:23.880569800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Create rdd of cleaned text without unknown words\n",
    "cleaned_rdd_with_split_text = dataset_cleaned.rdd.map(lambda obj: {\"label\": obj[\"label\"], \"text\": \" \".join(kp.replace_keywords(obj[\"text\"]).split())}).map(lambda obj: {\"label\": obj[\"label\"], \"text\": kp_special_tokens.replace_keywords(obj[\"text\"])})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:23:45.880217800Z",
     "start_time": "2023-07-24T22:23:45.839208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Compute list of words with number of occurrences\n",
    "words = cleaned_rdd_with_split_text.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(obj[\"text\"], True)]).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > 1).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:25:04.247227Z",
     "start_time": "2023-07-24T22:23:59.676219400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "save_list_to_csv(sorted(words, key=lambda x: x[1]), directory + \"/data/sentiment-knowledge/twitter-occurrences.csv\", [\"word\", \"occurrences\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:25:12.151456100Z",
     "start_time": "2023-07-24T22:25:12.093661200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning (removal of uncommon words and ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Get words with less than 10 occurrences\n",
    "words_to_analyze = cleaned_rdd_with_split_text.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(obj[\"text\"], True)]).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] <= 10).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:26:33.622761100Z",
     "start_time": "2023-07-24T22:25:31.731894700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Remove words with one occurrence and words with less than 10 occurrences containing both numbers and characters\n",
    "kp_words_to_remove = KeywordProcessor()\n",
    "\n",
    "for word in words_to_analyze:\n",
    "    if word[1] == 1:\n",
    "        kp_words_to_remove.add_keyword(word[0], \" \")\n",
    "    elif word[1] <= 10:\n",
    "        if contains_numbers(word[0]) and not contains_numbers_and_x(word[0]):\n",
    "            kp_words_to_remove.add_keyword(word[0], \" \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:26:37.680638700Z",
     "start_time": "2023-07-24T22:26:37.392384900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "cleaned_rdd_final = cleaned_rdd_with_split_text.map(lambda obj: {\"label\": obj[\"label\"], \"text\": \" \".join(kp_words_to_remove.replace_keywords(obj[\"text\"]).split())})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:26:52.117878100Z",
     "start_time": "2023-07-24T22:26:52.104878900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Save final dataset\n",
    "save_rdd_to_json_file(path_tweets_no_unknown_directory, cleaned_rdd_final, [\"label\", \"text\"])\n",
    "\n",
    "# Create dataset from files\n",
    "merge_files(path_tweets_no_unknown_directory, path_tweets_no_unknown)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T22:27:27.135604Z",
     "start_time": "2023-07-24T22:26:54.777144500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
