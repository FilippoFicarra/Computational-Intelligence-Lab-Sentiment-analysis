{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset extractor\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import save_rdd_to_json_file, merge_files\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:10.664388500Z",
     "start_time": "2023-07-20T14:05:09.067606500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define paths\n",
    "path_complete_dataset = directory + \"/data/datasets/amazon-reviews\"\n",
    "path_merged_dataset = directory + \"/data/datasets/amazon.json\"\n",
    "path_dataset_directory = directory + \"/data/datasets/amazon\"\n",
    "# Define number of examples to take and limit for number of characters.\n",
    "limit = 1000000\n",
    "upper_limit_characters = 256\n",
    "lower_limit_characters = 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:14.926113200Z",
     "start_time": "2023-07-20T14:05:14.872669500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:16.316495600Z",
     "start_time": "2023-07-20T14:05:16.302492200Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xmx32g -Xms12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset = spark.read.json(path_complete_dataset, schema=\"overall float, reviewText string\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:21.937690800Z",
     "start_time": "2023-07-20T14:05:20.237269900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:21:55.779931Z",
     "start_time": "2023-07-20T14:05:27.686974900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of items with overall 1\n",
    "dataset1 = dataset.rdd.filter(lambda obj: obj[\"overall\"] == 1.0 and obj[\"reviewText\"] is not None)\\\n",
    "    .filter(lambda obj: lower_limit_characters <= len(obj[\"reviewText\"]) <= upper_limit_characters)\\\n",
    "    .map(lambda obj: {\"label\": int(obj[\"overall\"]), \"text\": obj[\"reviewText\"]})\n",
    "\n",
    "# Estimate the total count of elements in the RDD\n",
    "estimated_count = dataset1.countApprox(timeout=100, confidence=0.95)\n",
    "\n",
    "# Calculate the fraction based on the desired sample size and estimated count\n",
    "fraction = min(limit / estimated_count, 1.0)\n",
    "\n",
    "# Sample items and save file\n",
    "save_rdd_to_json_file(path_dataset_directory + \"/dataset1\", dataset1.sample(False, fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T14:48:15.602037400Z",
     "start_time": "2023-07-20T14:30:43.536222200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of items with overall 2\n",
    "dataset2 = dataset.rdd.filter(lambda obj: obj[\"overall\"] == 2.0 and obj[\"reviewText\"] is not None)\\\n",
    "    .filter(lambda obj: lower_limit_characters <= len(obj[\"reviewText\"].replace(\" \", \"\")) <= upper_limit_characters)\\\n",
    "    .map(lambda obj: {\"label\": int(obj[\"overall\"]), \"text\": obj[\"reviewText\"]})\n",
    "\n",
    "# Estimate the total count of elements in the RDD\n",
    "estimated_count = dataset2.countApprox(timeout=100, confidence=0.95)\n",
    "\n",
    "# Calculate the fraction based on the desired sample size and estimated count\n",
    "fraction = min(limit / estimated_count, 1.0)\n",
    "\n",
    "# Sample items and save file\n",
    "save_rdd_to_json_file(path_dataset_directory + \"/dataset2\", dataset2.sample(False, fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T15:07:57.934555200Z",
     "start_time": "2023-07-20T14:49:34.358989200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of items with overall 3\n",
    "dataset3 = dataset.rdd.filter(lambda obj: obj[\"overall\"] == 3.0 and obj[\"reviewText\"] is not None)\\\n",
    "    .filter(lambda obj: lower_limit_characters <= len(obj[\"reviewText\"].replace(\" \", \"\")) <= upper_limit_characters)\\\n",
    "    .map(lambda obj: {\"label\": int(obj[\"overall\"]), \"text\": obj[\"reviewText\"]})\n",
    "\n",
    "# Estimate the total count of elements in the RDD\n",
    "estimated_count = dataset3.countApprox(timeout=100, confidence=0.95)\n",
    "\n",
    "# Calculate the fraction based on the desired sample size and estimated count\n",
    "fraction = min(limit / estimated_count, 1.0)\n",
    "\n",
    "# Sample items and save file\n",
    "save_rdd_to_json_file(path_dataset_directory + \"/dataset3\", dataset3.sample(False, fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T15:26:33.234964800Z",
     "start_time": "2023-07-20T15:08:27.739969600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of items with overall 4\n",
    "dataset4 = dataset.rdd.filter(lambda obj: obj[\"overall\"] == 4.0 and obj[\"reviewText\"] is not None)\\\n",
    "    .filter(lambda obj: lower_limit_characters <= len(obj[\"reviewText\"].replace(\" \", \"\")) <= upper_limit_characters)\\\n",
    "    .map(lambda obj: {\"label\": int(obj[\"overall\"]), \"text\": obj[\"reviewText\"]})\n",
    "\n",
    "# Estimate the total count of elements in the RDD\n",
    "estimated_count = dataset4.countApprox(timeout=100, confidence=0.95)\n",
    "\n",
    "# Calculate the fraction based on the desired sample size and estimated count\n",
    "fraction = min(limit / estimated_count, 1.0)\n",
    "\n",
    "# Sample items and save file\n",
    "save_rdd_to_json_file(path_dataset_directory + \"/dataset4\", dataset4.sample(False, fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T15:45:16.181575300Z",
     "start_time": "2023-07-20T15:26:33.235962400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of items with overall 5\n",
    "dataset5 = dataset.rdd.filter(lambda obj: obj[\"overall\"] == 5.0 and obj[\"reviewText\"] is not None)\\\n",
    "    .filter(lambda obj: lower_limit_characters <= len(obj[\"reviewText\"].replace(\" \", \"\")) <= upper_limit_characters)\\\n",
    "    .map(lambda obj: {\"label\": int(obj[\"overall\"]), \"text\": obj[\"reviewText\"]})\n",
    "\n",
    "# Estimate the total count of elements in the RDD\n",
    "estimated_count = dataset5.countApprox(timeout=100, confidence=0.95)\n",
    "\n",
    "# Calculate the fraction based on the desired sample size and estimated count\n",
    "fraction = min(limit / estimated_count, 1.0)\n",
    "\n",
    "# Sample items and save file\n",
    "save_rdd_to_json_file(path_dataset_directory + \"/dataset5\", dataset5.sample(False, fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create dataset from files produced by previous cells\n",
    "merge_files(path_dataset_directory, path_merged_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:50:51.829517Z",
     "start_time": "2023-07-20T15:50:34.113331800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
