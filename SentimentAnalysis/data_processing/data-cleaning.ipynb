{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaner for reviews dataset\n",
    "Disclaymer: To run this notebook, launch pyspark (command \"pyspark --master local[*number of cores*]\") from the folder containing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T11:05:38.446036200Z",
     "start_time": "2023-07-24T11:05:34.147438100Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import *\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T11:05:39.304314700Z",
     "start_time": "2023-07-24T11:05:39.284310300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get directory\n",
    "directory = os.path.dirname(os.getcwd()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Define paths\n",
    "path = directory + \"/data/datasets/amazon.json\"\n",
    "path_cleaned = directory + \"/data/datasets/amazon-cleaned.json\"\n",
    "path_cleaned_directory = directory + \"/data/datasets/amazon-cleaned\"\n",
    "path_cleaned_unknown = directory + \"/data/datasets/amazon-cleaned-no-unknown.json\"\n",
    "path_cleaned_unknown_directory = directory + \"/data/datasets/amazon-cleaned-no-unknown\"\n",
    "path_all_occurrences = directory + \"/data/sentiment-knowledge/amazon-all-words-with-occurrences.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:07:59.981902100Z",
     "start_time": "2023-07-22T20:07:59.960773300Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"1200s\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xmx32g -Xms12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:08:05.466407300Z",
     "start_time": "2023-07-22T20:08:01.021114100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = spark.read.json(path)\n",
    "\n",
    "# Split rdd into multiple rdds\n",
    "split_rdds = dataset.randomSplit([0.1 for _ in range(0,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:08:07.005649700Z",
     "start_time": "2023-07-22T20:08:06.086122700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of cleaned text\n",
    "cleaned_rdds = []\n",
    "\n",
    "for df in split_rdds:\n",
    "    cleaned_rdds.append(\n",
    "        df.rdd.distinct().filter(lambda obj: obj[\"label\"] != 3)\n",
    "            .map(lambda obj: {\"label\": obj[\"label\"], \"text\": obj[\"text\"].lower()})\n",
    "            .filter(lambda obj: \"old review\" not in obj[\"text\"]) # Remove objects containing \"old review\"\n",
    "            .map(lambda obj: {\"label\": obj[\"label\"], \"text\": cleaning_function_no_unknown(obj[\"text\"])}) # Clean\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:13:18.394816300Z",
     "start_time": "2023-07-22T20:08:08.539687900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save cleaned dataset with unknown words\n",
    "for i in range(0, len(cleaned_rdds)):\n",
    "    save_rdd_to_json_file(path_cleaned_directory + \"/cleaned\" + f\"{i}\", cleaned_rdds[i])\n",
    "\n",
    "# Create dataset from files\n",
    "merge_files(path_cleaned_directory, path_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:16:38.579628700Z",
     "start_time": "2023-07-22T20:16:37.925864600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "dataset_cleaned = spark.read.json(path_cleaned)\n",
    "\n",
    "# Split rdd into multiple rdds\n",
    "split_rdds_cleaned = dataset_cleaned.randomSplit([0.1 for _ in range(0,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:16:41.646085900Z",
     "start_time": "2023-07-22T20:16:40.997725700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute list of words with number of occurrences\n",
    "words_occurrences_rdds = []\n",
    "\n",
    "for df in split_rdds_cleaned:\n",
    "    words_occurrences_rdds.append(\n",
    "        df.rdd.flatMap(lambda obj: [(word, 1) for word in tokenize_with_sequences(remove_symbols_before_tokenization(obj[\"text\"]))]) # Tokenize\n",
    "            .reduceByKey(lambda x, y: x + y) # Add occurrences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:19:54.833691500Z",
     "start_time": "2023-07-22T20:16:42.802978300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge rdds, then reduce by key to obtain final vocabulary with number of occurreces\n",
    "merged_rdd_occurrences = words_occurrences_rdds[0].union(words_occurrences_rdds[1])\n",
    "\n",
    "for i in range(2, len(words_occurrences_rdds)):\n",
    "    merged_rdd_occurrences = merged_rdd_occurrences.union(words_occurrences_rdds[i])\n",
    "\n",
    "words_with_occurrences = merged_rdd_occurrences.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:20:11.759178400Z",
     "start_time": "2023-07-22T20:20:10.712112900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of retained words is 99.74917056838864\n"
     ]
    }
   ],
   "source": [
    "# Process list of words with occurrences\n",
    "count_unknown = 0\n",
    "count_frequent = 0\n",
    "frequent_words = []\n",
    "unknown_words = []\n",
    "for word in sorted(words_with_occurrences, key=lambda x: x[1]):\n",
    "    if word[1] <= 3:\n",
    "        count_unknown += word[1]\n",
    "        unknown_words.append(word[0])\n",
    "    else:\n",
    "        count_frequent += word[1]\n",
    "        frequent_words.append(word)\n",
    "\n",
    "# Print percentage of retained words\n",
    "print(f\"The percentage of retained words is {(count_frequent * 100)/(count_frequent + count_unknown)}\")\n",
    "\n",
    "# Create keyword processor for later use\n",
    "kp = KeywordProcessor()\n",
    "for word in unknown_words:\n",
    "    kp.add_keyword(word, ' ')\n",
    "\n",
    "# Save dataframe with occurrences of known words for later use\n",
    "save_list_to_csv(sorted(frequent_words, key=lambda x: x[1]), path_all_occurrences, [\"word\", \"occurrences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:21:08.475841400Z",
     "start_time": "2023-07-22T20:21:08.448409700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rdd of cleaned text without unknown words\n",
    "cleaned_no_unknown_rdds = []\n",
    "\n",
    "for df in split_rdds_cleaned:\n",
    "    cleaned_no_unknown_rdds.append(\n",
    "        df.rdd.map(lambda obj: {\"label\": obj[\"label\"], \"text\": \" \".join(kp.replace_keywords(obj[\"text\"]).split())}) # Remove unknown\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T20:22:58.510531700Z",
     "start_time": "2023-07-22T20:21:11.139530800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save cleaned dataset with unknown words\n",
    "for i in range(0, len(cleaned_no_unknown_rdds)):\n",
    "    save_rdd_to_json_file(path_cleaned_unknown_directory + \"/cleaned\" + f\"{i}\", cleaned_no_unknown_rdds[i])\n",
    "\n",
    "# Create dataset from files\n",
    "merge_files(path_cleaned_unknown_directory, path_cleaned_unknown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
